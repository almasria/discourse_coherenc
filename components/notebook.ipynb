{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertModel.forward() got an unexpected keyword argument 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello world\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\a.almasri\\OneDrive\\Germany\\Uni Heidelberg\\WS23.24\\Thesis\\GitHub\\text_coherence\\components\\model.py:38\u001b[0m, in \u001b[0;36mModel.embed\u001b[1;34m(self, sentences, normalzied)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Tokenize sentences\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Compute token embeddings\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\a.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\a.almasri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BertModel.forward() got an unexpected keyword argument 'padding'"
     ]
    }
   ],
   "source": [
    "m.embed(sentences=[\"Hello world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "# sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "sentences = [\"Hello world\"]\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentences: List[str] = None, normalzied: bool = True) -> torch.Tensor:\n",
    "\n",
    "    if sentences is not None:\n",
    "            \n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        if normalzied:\n",
    "            # Normalize embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4477e-02,  3.1023e-02,  6.7349e-03,  2.6109e-02, -3.9362e-02,\n",
      "         -1.6030e-01,  6.6924e-02, -6.4415e-03, -4.7451e-02,  1.4759e-02,\n",
      "          7.0875e-02,  5.5528e-02,  1.9193e-02, -2.6251e-02, -1.0110e-02,\n",
      "         -2.6941e-02,  2.2308e-02, -2.2227e-02, -1.4969e-01, -1.7493e-02,\n",
      "          7.6763e-03,  5.4352e-02,  3.2544e-03,  3.1726e-02, -8.4621e-02,\n",
      "         -2.9406e-02,  5.1596e-02,  4.8124e-02, -3.3148e-03, -5.8279e-02,\n",
      "          4.1969e-02,  2.2211e-02,  1.2819e-01, -2.2339e-02, -1.1656e-02,\n",
      "          6.2928e-02, -3.2876e-02, -9.1226e-02, -3.1175e-02,  5.2700e-02,\n",
      "          4.7035e-02, -8.4203e-02, -3.0056e-02, -2.0745e-02,  9.5178e-03,\n",
      "         -3.7218e-03,  7.3434e-03,  3.9324e-02,  9.3274e-02, -3.7887e-03,\n",
      "         -5.2742e-02, -5.8058e-02, -6.8644e-03,  5.2832e-03,  8.2893e-02,\n",
      "          1.9363e-02,  6.2845e-03, -1.0331e-02,  9.0324e-03, -3.7684e-02,\n",
      "         -4.5206e-02,  2.4016e-02, -6.9442e-03,  1.3492e-02,  1.0006e-01,\n",
      "         -7.1684e-02, -2.1695e-02,  3.1618e-02, -5.1635e-02, -8.2248e-02,\n",
      "         -6.5693e-02, -9.8954e-03,  5.8164e-03,  7.3555e-02, -3.4050e-02,\n",
      "          2.4886e-02,  1.4488e-02,  2.6457e-02,  9.6568e-03,  3.0217e-02,\n",
      "          5.2804e-02, -7.5360e-02,  9.8972e-03,  2.9837e-02,  1.7556e-02,\n",
      "          2.3092e-02,  1.9339e-03,  1.4002e-03, -4.7176e-02, -1.1194e-02,\n",
      "         -1.1420e-01, -1.9812e-02,  4.0266e-02,  2.1930e-03, -7.9792e-02,\n",
      "         -2.5382e-02,  9.4483e-02, -2.8981e-02, -1.4500e-01,  2.3098e-01,\n",
      "          2.7731e-02,  3.2111e-02,  3.1065e-02,  4.2833e-02,  6.4238e-02,\n",
      "          3.2163e-02, -4.8766e-03,  5.5699e-02, -3.7532e-02, -2.1506e-02,\n",
      "         -2.8343e-02, -2.8847e-02,  3.8353e-02, -1.7469e-02,  5.2485e-02,\n",
      "         -7.4876e-02, -3.1260e-02,  2.1842e-02, -3.9896e-02, -8.5871e-03,\n",
      "          2.6957e-02, -4.8496e-02,  1.1470e-02,  2.9618e-02, -2.0572e-02,\n",
      "          1.3104e-02,  2.8833e-02, -3.1942e-33,  6.4782e-02, -1.8130e-02,\n",
      "          5.1790e-02,  1.2198e-01,  2.8780e-02,  8.7220e-03, -7.0521e-02,\n",
      "         -1.6907e-02,  4.0740e-02,  4.2116e-02,  2.5447e-02,  3.5746e-02,\n",
      "         -4.9145e-02,  2.1291e-03, -1.5547e-02,  5.0731e-02, -4.8185e-02,\n",
      "          3.5881e-02, -4.0671e-03,  1.0172e-01, -5.5970e-02, -1.0681e-02,\n",
      "          1.1236e-02,  9.0687e-02,  4.2345e-03,  3.5139e-02, -9.7028e-03,\n",
      "         -9.3865e-02,  9.2856e-02,  8.0049e-03, -7.7055e-03, -5.2087e-02,\n",
      "         -1.2588e-02,  3.2669e-03,  6.0135e-03,  7.5817e-03,  1.0517e-02,\n",
      "         -8.6345e-02, -6.9879e-02, -2.5338e-03, -9.0977e-02,  4.6887e-02,\n",
      "          5.2077e-02,  7.1939e-03,  1.0904e-02, -5.2296e-03,  1.3937e-02,\n",
      "          2.1968e-02,  3.4209e-02,  6.0225e-02,  1.1662e-04,  1.4732e-02,\n",
      "         -7.0089e-02,  2.8499e-02, -2.7602e-02,  1.0768e-02,  3.4831e-02,\n",
      "         -2.2488e-02,  9.7691e-03,  7.7228e-02,  2.1588e-02,  1.1496e-01,\n",
      "         -6.8001e-02,  2.3761e-02, -1.5984e-02, -1.7827e-02,  6.4395e-02,\n",
      "          3.2026e-02,  5.0270e-02, -5.9137e-03, -3.3708e-02,  1.7840e-02,\n",
      "          1.6573e-02,  6.3297e-02,  3.4677e-02,  4.6474e-02,  9.7906e-02,\n",
      "         -6.6355e-03,  2.5207e-02, -7.7988e-02,  1.6926e-02, -9.4589e-04,\n",
      "          2.2472e-02, -3.8253e-02,  9.5705e-02, -5.3507e-03,  1.0469e-02,\n",
      "         -1.1524e-01, -1.3263e-02, -1.0709e-02, -8.3117e-02,  7.3274e-02,\n",
      "          4.9392e-02, -8.9944e-03, -9.5846e-02,  3.3661e-33,  1.2493e-01,\n",
      "          1.9350e-02, -5.8226e-02, -3.5988e-02, -5.0747e-02, -4.5662e-02,\n",
      "         -8.2603e-02,  1.4819e-01, -8.8421e-02,  6.0274e-02,  5.1030e-02,\n",
      "          1.0303e-02,  1.4121e-01,  3.0814e-02,  6.1033e-02, -5.2851e-02,\n",
      "          1.3665e-01,  9.1899e-03, -1.7325e-02, -1.2849e-02, -7.9953e-03,\n",
      "         -5.0980e-02, -5.2351e-02,  7.5930e-03, -1.5166e-02,  1.6960e-02,\n",
      "          2.1271e-02,  2.0558e-02, -1.2003e-01,  1.4462e-02,  2.6760e-02,\n",
      "          2.5331e-02, -4.2755e-02,  6.7685e-03, -1.4459e-02,  4.5262e-02,\n",
      "         -9.1477e-02, -1.9439e-02, -1.7833e-02, -5.4910e-02, -5.2641e-02,\n",
      "         -1.0459e-02, -5.2016e-02,  2.0892e-02, -7.9970e-02, -1.2111e-02,\n",
      "         -5.7731e-02,  2.3178e-02, -8.0316e-03, -2.5989e-02, -7.9957e-02,\n",
      "         -2.0729e-02,  4.8818e-02, -2.0389e-02, -4.9177e-02,  1.4160e-02,\n",
      "         -6.3622e-02, -7.8074e-03,  1.6432e-02, -2.5683e-02,  1.3381e-02,\n",
      "          2.6249e-02,  9.9784e-03,  6.3229e-02,  2.6721e-03, -6.5827e-03,\n",
      "          1.6632e-02,  3.2366e-02,  3.7942e-02, -3.6376e-02, -6.9109e-03,\n",
      "          1.5970e-04, -1.6336e-03, -2.7278e-02, -2.8038e-02,  4.9682e-02,\n",
      "         -2.8867e-02, -2.4180e-03,  1.4775e-02,  9.7646e-03,  5.7975e-03,\n",
      "          1.3486e-02,  5.5679e-03,  3.7227e-02,  7.2325e-03,  4.0156e-02,\n",
      "          8.1503e-02,  7.1992e-02, -1.3056e-02, -4.2882e-02, -1.1011e-02,\n",
      "          4.8978e-03, -9.2297e-03,  3.5192e-02, -5.1035e-02, -1.5714e-08,\n",
      "         -8.8624e-02,  2.3909e-02, -1.6239e-02,  3.1700e-02,  2.7284e-02,\n",
      "          5.2469e-02, -4.7071e-02, -5.8847e-02, -6.3208e-02,  4.0889e-02,\n",
      "          4.9828e-02,  1.0655e-01, -7.4502e-02, -1.2495e-02,  1.8371e-02,\n",
      "          3.9474e-02, -2.4798e-02,  1.4516e-02, -3.7069e-02,  2.0016e-02,\n",
      "         -4.8592e-05,  9.8666e-03,  2.4839e-02, -5.2458e-02,  2.9314e-02,\n",
      "         -8.7192e-02, -1.4500e-02,  2.6019e-02, -1.8746e-02, -7.6205e-02,\n",
      "          3.5043e-02,  1.0364e-01, -2.8051e-02,  1.2718e-02, -7.6325e-02,\n",
      "         -1.8652e-02,  2.4977e-02,  8.1445e-02,  6.8759e-02, -6.4057e-02,\n",
      "         -8.3894e-02,  6.1362e-02, -3.3546e-02, -1.0615e-01, -4.0081e-02,\n",
      "          3.2530e-02,  7.6625e-02, -7.3016e-02,  3.3757e-04, -4.0872e-02,\n",
      "         -7.5788e-02,  2.7528e-02,  7.4625e-02,  1.7717e-02,  9.1218e-02,\n",
      "          1.1022e-01,  5.6981e-04,  5.1463e-02, -1.4551e-02,  3.3232e-02,\n",
      "          2.3792e-02, -2.2890e-02,  3.8938e-02,  3.0207e-02]])\n"
     ]
    }
   ],
   "source": [
    "print(encode([\"Hello world\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from utils import mean_pooling\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"\n",
    "    A class to wrap the embedding model to be used\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    allowed_models = [\"all-mpnet-base-v2\",\n",
    "                      \"all-MiniLM-L6-v2\"]\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\") -> None:\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        if model_name in self.allowed_models:\n",
    "            # Load model from HuggingFace Hub\n",
    "            self.model = AutoTokenizer.from_pretrained(\n",
    "                \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            )\n",
    "            self.tokenizer = AutoModel.from_pretrained(\n",
    "                \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Model not supported!\")\n",
    "\n",
    "    def emb(self, sentences: List[str] = None, normalzied: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if sentences is not None:\n",
    "            # Tokenize sentences\n",
    "            encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "            # Compute token embeddings\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**encoded_input)\n",
    "\n",
    "            # Perform pooling\n",
    "            sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "            if normalzied:\n",
    "                # Normalize embeddings\n",
    "                sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "            return sentence_embeddings           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EmbeddingModel(model_name=\"all-MiniLM-L6-v2\")\n",
    "m.emb([\"Hello world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from utils import mean_pooling\n",
    "\n",
    "\n",
    "\n",
    "def embed(sentences, normalzied):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "   \n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input) \n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    if normalzied:\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0719,  0.0367, -0.0124,  ...,  0.0974, -0.0844, -0.0289],\n",
       "        [ 0.0844,  0.0285,  0.0038,  ...,  0.2837, -0.0755, -0.0455]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.embed([\"Hello world\", \"I am here\"], normalzied=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EmbeddingModel\n",
    "from utils import mean_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = m.embed([\"Hello world\", \"I am here\"], normalzied=False, return_tensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Hello world\", \"I am here\", \"I am here\", \"I am here\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EmbeddingModel(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QuerySession:\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "\n",
    "    def __init__(self, queries: List[str] = None, normalize_embeddings: bool = False, embedding_model: str = None) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if queries is not None:\n",
    "            self.queries: List[str] = queries[:]\n",
    "            self.session_size = len(queries)\n",
    "            self.embedding_model = embedding_model\n",
    "            self.normalized_session = normalize_embeddings\n",
    "            self.embeddings = self._embed_queries(queries=queries, embedding_model=self.embedding_model, normalize_embeddings=self.normalized_session)\n",
    "            self.global_coherence_score = self.compute_global_coherence(context_window=1)\n",
    "            self.context_window_size = 1\n",
    "                       \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def _embed_queries(self, queries: List[str], embedding_model: str = None, normalize_embeddings: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if embedding_model is not None:\n",
    "            return embedding_model.embed(sentences=queries, normalize_embeddings=normalize_embeddings)\n",
    "\n",
    "    def add_query(self, query) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.queries.append(query)\n",
    "        self.session_size += 1\n",
    "        query_embedding = self.embedding_model.embed(sentences=[query], normalize_embeddings=self.normalized_session)\n",
    "        print(query_embedding.shape)\n",
    "        self.embeddings = np.concatenate([self.embeddings , query_embedding])\n",
    "\n",
    "    def _compute_context_vector(self, position, context_window) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if context_window >= self.session_size:\n",
    "            raise ValueError(\"Context window is too big\")\n",
    "\n",
    "        if position < context_window:\n",
    "            context_vector = np.sum(self.embeddings[:position], axis=0)\n",
    "            context_vector = context_vector / np.linalg.norm(context_vector)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            context_vector = np.sum(self.embeddings[position-context_window:position], axis=0)\n",
    "            context_vector = context_vector / np.linalg.norm(context_vector)\n",
    "\n",
    "        return context_vector        \n",
    "            \n",
    "      \n",
    "        \n",
    "\n",
    "    def _compute_local_coherence(self, current_vector: np.ndarray = None, neighbor_vector: np.ndarray = None) -> float:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if current_vector is not None and neighbor_vector is not None:\n",
    "            return np.dot(current_vector, neighbor_vector) / (np.linalg.norm(current_vector) * np.linalg.norm(neighbor_vector))\n",
    "    \n",
    "        \n",
    "    def compute_global_coherence(self, context_window) -> float:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if self.session_size > 1:\n",
    "\n",
    "            coherence_score = 0.0\n",
    "\n",
    "            for i, embedding in enumerate(self.embeddings):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                context_vector = self._compute_context_vector(position=i, context_window=context_window)\n",
    "                coherence_score += self._compute_local_coherence(current_vector=embedding, neighbor_vector=context_vector)\n",
    "            \n",
    "        return coherence_score / (self.session_size - 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = QuerySession(queries=queries, embedding_model=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world', 'I am here', 'I am here', 'I am here']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7591788669427236"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.global_coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s._compute_global_coherence(context_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = embed(queries, normalzied=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7591788570086161"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_score = 0.0\n",
    "\n",
    "for i, embedding in enumerate(e):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    coherence_score += np.dot(embedding, e[i-1]) / (np.linalg.norm(embedding) * np.linalg.norm(e[i-1]))\n",
    "\n",
    "coherence_score / (len(e) - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
